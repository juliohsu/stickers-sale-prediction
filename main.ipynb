{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio-hsu/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647329220/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the cuda gpu status\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize, transform and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test data url\n",
    "train_data_url = \"./data/train.csv\"\n",
    "test_data_url = \"./data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training and testing dataframe\n",
    "train_df = pd.read_csv(train_data_url)\n",
    "test_df = pd.read_csv(test_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "date           0\n",
       "country        0\n",
       "store          0\n",
       "product        0\n",
       "num_sold    8871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "date       0\n",
       "country    0\n",
       "store      0\n",
       "product    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18989/2387463465.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/tmp/ipykernel_18989/2387463465.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mode(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "date        0\n",
       "country     0\n",
       "store       0\n",
       "product     0\n",
       "year        0\n",
       "month       0\n",
       "weekYear    0\n",
       "dayWeek     0\n",
       "dayYear     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date engineering for the training and testing set dataframes\n",
    "\n",
    "def fill_nan(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in [\"int64\", \"float64\"]:\n",
    "            df[column].fillna(df[column].mean(), inplace=True)\n",
    "        else:\n",
    "            df[column].fillna(df[column].mode(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def convert_date(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"weekYear\"] = df[\"date\"].dt.isocalendar().week\n",
    "    df[\"dayWeek\"] = df[\"date\"].dt.dayofweek\n",
    "    df[\"dayYear\"] = df[\"date\"].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "train_df = fill_nan(train_df)\n",
    "\n",
    "train_df = convert_date(train_df)\n",
    "test_df = convert_date(test_df)\n",
    "\n",
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out a single training set into multiple training set (train & validation dataframe)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_df = shuffle(val_df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([184104, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing libraries ABV\n",
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "# filter out unnecessary features for training and validation set\n",
    "def colLabelEnconder(X):\n",
    "    columns = [\"country\", \"store\", \"product\"]\n",
    "    for col in columns:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "    return X\n",
    "\n",
    "# training\n",
    "train_X = train_df.drop(columns=[\"id\", \"date\", \"num_sold\"])\n",
    "train_X_encoded = colLabelEnconder(train_X)\n",
    "#train_X_encoded = pd.get_dummies(train_X, columns=[\"country\", \"store\", \"product\"]).values.astype(\"float32\")\n",
    "train_X_scaled = scaler.fit_transform(train_X_encoded)\n",
    "train_X_tensor = torch.tensor(train_X_scaled, dtype=torch.float32)\n",
    "\n",
    "train_y = train_df[\"num_sold\"].values.astype(\"float32\")\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "train_td = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "train_loader = DataLoader(train_td, batch_size=16, shuffle=True)\n",
    "\n",
    "# validation\n",
    "val_X = val_df.drop(columns=[\"id\", \"date\", \"num_sold\"])\n",
    "val_X_encoded = colLabelEnconder(val_X)\n",
    "#val_X_encoded = pd.get_dummies(val_X, columns=[\"country\", \"store\", \"product\"]).values.astype(\"float32\")\n",
    "val_X_scaled = scaler.fit_transform(val_X_encoded)\n",
    "val_X_tensor = torch.tensor(val_X_scaled, dtype=torch.float32)\n",
    "\n",
    "val_y = val_df[\"num_sold\"].values.astype(\"float32\")\n",
    "val_y_tensor = torch.tensor(val_y, dtype=torch.float32)\n",
    "\n",
    "val_td = TensorDataset(val_X_tensor, val_y_tensor)\n",
    "val_loader = DataLoader(val_td, batch_size=16, shuffle=False)\n",
    "\n",
    "train_X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98550, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# align testing features with training set features\n",
    "test_X = test_df.drop(columns=[\"id\", \"date\"])\n",
    "test_X_encoded = colLabelEnconder(test_X)\n",
    "#test_X_encoded = pd.get_dummies(test_X, columns=[\"country\", \"store\", \"product\"]).values.astype(\"float32\")\n",
    "test_X_scaled = scaler.fit_transform(test_X_encoded)\n",
    "test_X_tensor = torch.tensor(test_X_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "test_X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self construct and define AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self implement linear regression model\n",
    "class lrModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lrModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(19, 8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# self implement gru model\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        # gate\n",
    "        self.Wz = nn.Linear(input_size, hidden_size)\n",
    "        self.Uz = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Wr = nn.Linear(input_size, hidden_size)\n",
    "        self.Ur = nn.Linear(hidden_size, hidden_size)\n",
    "        # hidden\n",
    "        self.Wh = nn.Linear(input_size, hidden_size)\n",
    "        self.Uh = nn.Linear(hidden_size, hidden_size)\n",
    "    def forward(self, x, h_prev):\n",
    "        z = torch.sigmoid(self.Wz(x) + self.Uz(h_prev))\n",
    "        r = torch.sigmoid(self.Wr(x) + self.Ur(h_prev))\n",
    "        h_tilde = torch.tanh(self.Wh(x) + self.Uh(r * h_prev))\n",
    "        h = (1 - z) * h_prev + z * h_tilde\n",
    "        return h\n",
    "    \n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        # list of grucells\n",
    "        self.grucells = nn.ModuleList([GRUCell(input_size, hidden_size) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = [torch.zeros(batch_size, self.hidden_size).to(device) for _ in range(self.num_layers)]\n",
    "        for t in range(seq_len):\n",
    "            for i in range(self.num_layers):\n",
    "                h[i] = self.grucells[i](x[:, t, :], h[i])\n",
    "        out = self.fc(h[-1])\n",
    "        return out\n",
    "    \n",
    "class GRUNetwork2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUNetwork2, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# self implement mape function\n",
    "def mape(y_pred, y):\n",
    "    y = torch.where(y == 0, torch.tensor(1e-10, dtype=torch.float32, device=y.device), y)\n",
    "    loss = torch.mean(torch.abs((y - y_pred) / y)) * 100\n",
    "    return loss\n",
    "\n",
    "# self implement mae function\n",
    "def mae(y_pred, y):\n",
    "    return torch.mean(torch.abs(y - y_pred))\n",
    "\n",
    "# self implement mse function\n",
    "def mse(y_pred, y):\n",
    "    return torch.mean((y - y_pred)**2)\n",
    "\n",
    "# define model and its parameters\n",
    "# model = lrModel().to(device)\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "model = GRUNetwork2(input_size, hidden_size, output_size, num_layers)\n",
    "optim = optimizer.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, losses 5294610947.9922\n",
      "epoch 1, losses 5281498193.3359\n",
      "epoch 2, losses 5273941064.5000\n",
      "epoch 3, losses 5269566111.4922\n",
      "epoch 4, losses 5267671804.4609\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if len(data.size()) == 2:\n",
    "            data = data.unsqueeze(1)\n",
    "        preds = model(data)\n",
    "        loss = mse(preds, labels)\n",
    "        losses += loss.item()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch {epoch}, losses {losses:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 46026 and correct 0\n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "model.eval()\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if len(data.size()) == 2:\n",
    "            data = data.unsqueeze(1)\n",
    "        preds = model(data)\n",
    "        _, predictions = torch.max(preds, 1)\n",
    "        correct_preds += (predictions == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "    print(f\"Total {total_preds} and correct {correct_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 184104, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 752.192731\n",
      "Total 46026 and correct 18235\n"
     ]
    }
   ],
   "source": [
    "# lgbm model\n",
    "model2 = lgbm.LGBMRegressor()\n",
    "model2.fit(train_X, train_y)\n",
    "y_pred2 = model2.predict(val_X)\n",
    "loss = mean_squared_error(y_pred2, val_y)\n",
    "prediction2 = np.sum(np.abs((val_y - y_pred2) / val_y) <= 0.05)\n",
    "print(f\"Total {len(val_y)} and correct {prediction2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
